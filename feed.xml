<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.willhwthompson.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.willhwthompson.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-28T05:09:24+00:00</updated><id>https://www.willhwthompson.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Generalization Error via the Replica Method: Learning in the Hidden Manifold Model</title><link href="https://www.willhwthompson.com/blog/2024/generalization-error-replica/" rel="alternate" type="text/html" title="Generalization Error via the Replica Method: Learning in the Hidden Manifold Model"/><published>2024-12-02T12:00:00+00:00</published><updated>2024-12-02T12:00:00+00:00</updated><id>https://www.willhwthompson.com/blog/2024/generalization-error-replica</id><content type="html" xml:base="https://www.willhwthompson.com/blog/2024/generalization-error-replica/"><![CDATA[<p>This post is an annotated writeup of a final project for <em>Information and Physical Computing</em> (CSDS/PHYS). The central question: in the high-dimensional limit (features $p$, samples $n$, and latent dimension $d$ all large with fixed ratios), what is the <em>exact</em> generalization error of a linear student model trained on data generated by a nonlinear teacher?</p> <p>The answer—derived via the replica method—reveals the celebrated <strong>double-descent</strong> phenomenon as a consequence of the structure of the loss landscape near the interpolation threshold $p/n \approx 1$.</p> <h3 id="setup-hidden-manifold-model">Setup: Hidden Manifold Model</h3> <p>Data is generated by a two-step process:</p> <ol> <li>Draw a low-dimensional latent vector $\mathbf{c}^\mu \sim \mathcal{N}(0, I_d)$.</li> <li>Project and apply a nonlinearity to get observations: $\mathbf{x}^\mu = \sigma!\left(\frac{1}{\sqrt{d}} F^\top \mathbf{c}^\mu\right)$, where $F \in \mathbb{R}^{d \times p}$ is a random matrix.</li> <li>Generate labels via a teacher direction $\theta^0$: $y^\mu = f^0!\left(\frac{1}{d}\,\mathbf{c}^\mu \cdot \theta^0\right)$.</li> </ol> <p>This “hides” the linear structure inside a random nonlinear projection. The student only sees $(\mathbf{x}^\mu, y^\mu)$ and must recover the direction $\theta^0$ in latent space.</p> <h3 id="key-results">Key Results</h3> <p>The generalization error in the thermodynamic limit depends only on three scalar <strong>overlap parameters</strong> ($m_s, q_s, q_w$) which satisfy self-consistent saddle-point equations. For regression:</p> \[\epsilon_g = \rho + Q^\star - 2M^\star\] <p>where $Q^\star = \kappa_1^2 q_s^\star + \kappa_\star^2 q_w^\star$ and $M^\star = \kappa_1 m_s^\star$. For classification, it reduces to the angle between the student and teacher directions:</p> \[\epsilon_g = \frac{1}{\pi}\cos^{-1}\!\left(\frac{M^\star}{\sqrt{Q^\star}}\right)\] <p>The double-descent spike at $p/n \approx 1$ is visible in the fixed-point solutions when regularization $\lambda$ is small.</p> <h3 id="double-descent">Double Descent</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/posts/generalization-error/figures/double_descent-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/posts/generalization-error/figures/double_descent-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/posts/generalization-error/figures/double_descent-1400.webp"/> <img src="/assets/posts/generalization-error/figures/double_descent.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Double descent curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Double descent: test error peaks near the interpolation threshold p/n ≈ 1, then decreases as the model becomes overparameterized.</figcaption> </figure> <h3 id="interactive-notebook">Interactive Notebook</h3> <p>The full derivation (Gibbs measure → replica trick → Gaussian equivalence → replica-symmetric saddle point → fixed-point iteration) is in the interactive notebook below. You can explore how regularization $\lambda$, the nonlinearity $\sigma$, and the noise level $\Delta$ affect the generalization error surface.</p> <div style="position: relative; width: 100%; padding-top: 10px; margin-bottom: 20px;"> <iframe src="/assets/posts/generalization-error/index.html" width="100%" height="900px" frameborder="0" style="border: 1px solid #e0e0e0; border-radius: 4px;" title="Generalization Error Interactive Notebook" loading="lazy"></iframe> </div> <p class="text-muted" style="font-size: 0.9em;"> <em>If the interactive notebook does not load, open it directly: <a href="/assets/posts/generalization-error/index.html">full notebook</a>.</em> </p> <h3 id="reference">Reference</h3> <p>Gerace, F., Loureiro, B., Krzakala, F., Mézard, M., &amp; Zdeborová, L. (2021). <em>Generalisation error in learning with random features and the hidden manifold model.</em> ICML 2021.</p>]]></content><author><name></name></author><category term="research"/><category term="machine-learning"/><category term="statistical-physics"/><category term="random-matrix"/><summary type="html"><![CDATA[A statistical-physics treatment of high-dimensional learning. We derive exact asymptotic generalization error for ridge regression and logistic regression on data generated from a hidden manifold model, using the replica method from disordered systems.]]></summary></entry></feed>